from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from pydantic_settings import BaseSettings
import os
from supabase import create_client, Client
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from typing import List, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor

# --- Configuration using Pydantic Settings ---
class Settings(BaseSettings):
    supabase_url: str
    supabase_key: str
    gemini_api_key: str
    host: str = "0.0.0.0"
    port: int = 8000

    class Config:
        env_file = ".env"

settings = Settings()

# --- Client Initialization ---
supabase: Client = create_client(settings.supabase_url, settings.supabase_key)

genai.configure(api_key=settings.gemini_api_key)

# Gemini 모델 초기화
try:
    gemini_model = genai.GenerativeModel('gemini-1.5-flash')
except Exception as e:
    print(f"gemini-1.5-flash 사용 불가, gemini-1.5-pro 시도: {e}")
    try:
        gemini_model = genai.GenerativeModel('gemini-1.5-pro')
    except Exception as e:
        print(f"gemini-1.5-pro도 사용 불가, 기본 모델 시도: {e}")
        gemini_model = genai.GenerativeModel('gemini-pro')

# 임베딩 모델 로드
print("Loading embedding model...")
embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
print("Embedding model loaded.")

app = FastAPI()

# --- Middleware ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class QueryInput(BaseModel):
    question: str
    top_k: Optional[int] = 3

class QueryResponse(BaseModel):
    answer: str
    relevant_documents: List[dict]

# --- Thread Pool for CPU-bound tasks ---
executor = ThreadPoolExecutor(max_workers=os.cpu_count())

# --- Helper Functions ---
def get_embedding(text: str) -> List[float]:
    """텍스트를 벡터로 변환"""
    return embedding_model.encode(text).tolist()

async def check_available_models():
    """사용 가능한 Gemini 모델 목록 확인"""
    try:
        models = genai.list_models()
        available_models = []
        for model in models:
            if 'generateContent' in model.supported_generation_methods:
                available_models.append(model.name)
        return available_models
    except Exception as e:
        print(f"모델 목록 조회 실패: {e}")
        return []

# --- API Endpoints ---
@app.on_event("startup")
async def startup_event():
    """서버 시작시 안내 메시지 출력"""
    print("=" * 60)
    print("🤖 RAG 질의응답 시스템이 시작되었습니다!")
    print("=" * 60)
    
    # 사용 가능한 모델 확인
    available_models = await check_available_models()
    if available_models:
        print("✅ 사용 가능한 Gemini 모델:")
        for model in available_models[:3]:  # 처음 3개만 표시
            print(f"   - {model}")
    
    print("\n📋 Supabase 설정 확인:")
    print("   - 'documents' 테이블과 'vector' extension이 활성화되어 있는지 확인하세요.")
    print("   - 필요한 경우 다음 SQL을 실행하세요:")
    print("""
-- 1. pg_vector extension 활성화
CREATE EXTENSION IF NOT EXISTS vector;

-- 2. documents 테이블 생성
CREATE TABLE IF NOT EXISTS documents (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    content TEXT NOT NULL,
    embedding VECTOR(768),
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- 3. 벡터 검색을 위한 함수 생성
CREATE OR REPLACE FUNCTION match_documents (
  query_embedding VECTOR(768),
  match_threshold FLOAT,
  match_count INT
)
RETURNS TABLE (
  id BIGINT,
  content TEXT,
  metadata JSONB,
  similarity FLOAT
)
LANGUAGE sql STABLE
AS $$
  SELECT
    d.id,
    d.content,
    d.metadata,
    1 - (d.embedding <=> query_embedding) AS similarity
  FROM documents d
  WHERE 1 - (d.embedding <=> query_embedding) > match_threshold
  ORDER BY similarity DESC
  LIMIT match_count;
$$;
    """)
    print("\n🌐 서버 실행:")
    print(f"   - Backend: http://localhost:{settings.port}")
    print("   - Frontend: index.html 파일을 브라우저에서 열어주세요")
    print("=" * 60)

@app.post("/query", response_model=QueryResponse)
async def query_documents(query: QueryInput):
    """질문에 대한 답변 생성"""
    try:
        # 1. 질문을 벡터로 변환
        current_loop = asyncio.get_running_loop()
        query_embedding = await current_loop.run_in_executor(executor, get_embedding, query.question)
        
        # 2. Supabase에서 관련 문서 검색
        try:
            response = supabase.rpc('match_documents', {
                'query_embedding': query_embedding,
                'match_threshold': 0.3,  # 임계값을 낮춰서 더 많은 결과 반환
                'match_count': query.top_k
            }).execute()
            
            top_docs = response.data if response.data else []
        except Exception as db_error:
            print(f"데이터베이스 검색 오류: {db_error}")
            # 데이터베이스 오류 시 빈 문서 리스트로 처리
            top_docs = []
        
        # 3. 문서가 없는 경우 일반 AI 응답 제공
        if not top_docs:
            print("관련 문서를 찾을 수 없어 일반 AI 응답을 제공합니다.")
            
            # 일반적인 질문에 대한 AI 응답
            general_prompt = f"""당신은 도움이 되는 AI 어시스턴트입니다.
사용자의 질문에 정확하고 유용한 답변을 한국어로 제공해주세요.

질문: {query.question}

답변:"""
            
            try:
                gemini_response = gemini_model.generate_content(general_prompt)
                answer_text = gemini_response.text
            except Exception as gemini_error:
                print(f"Gemini API 오류: {gemini_error}")
                answer_text = "죄송합니다. 현재 AI 서비스에 일시적인 문제가 발생했습니다. 잠시 후 다시 시도해주세요."
            
            return QueryResponse(
                answer=answer_text,
                relevant_documents=[]
            )
        
        # 4. 관련 문서가 있는 경우 컨텍스트 기반 응답
        context = "\n\n---\n\n".join([doc['content'] for doc in top_docs])
        
        # RAG 기반 프롬프트
        rag_prompt = f"""당신은 주어진 문서 내용을 바탕으로 질문에 답변하는 AI 어시스턴트입니다.

**문서 내용:**
{context}

**질문:**
{query.question}

**지침:**
- 반드시 위에 주어진 '문서 내용'만을 참고하여 답변하세요.
- 문서에 내용이 없다면, "문서에 관련된 내용이 없습니다."라고 답변하세요.
- 답변은 명확하고 간결하게 한국어로 작성해주세요.
- 가능하면 구체적인 정보를 포함해주세요.

**답변:**
"""
        
        try:
            gemini_response = gemini_model.generate_content(rag_prompt)
            answer_text = gemini_response.text
        except Exception as gemini_error:
            print(f"Gemini API 오류: {gemini_error}")
            answer_text = f"AI 응답 생성 중 오류가 발생했습니다. 참고 문서 내용을 확인해주세요.\n\n참고 문서:\n{context[:300]}..."
        
        return QueryResponse(
            answer=answer_text,
            relevant_documents=top_docs
        )
        
    except Exception as e:
        print(f"Query 처리 오류: {e}")
        # 전체 오류 시 기본 응답 제공
        return QueryResponse(
            answer="죄송합니다. 현재 서비스에 일시적인 문제가 발생했습니다. 잠시 후 다시 시도해주세요.",
            relevant_documents=[]
        )

@app.get("/models")
async def get_available_models():
    """사용 가능한 Gemini 모델 목록 반환"""
    try:
        available_models = await check_available_models()
        return {"available_models": available_models}
    except Exception as e:
        return {"error": f"모델 목록 조회 실패: {str(e)}"}

@app.get("/health")
async def health_check():
    """서버 상태 확인"""
    try:
        # Supabase 연결 테스트
        supabase.table("documents").select("count", count="exact").limit(1).execute()
        db_status = "healthy"
    except:
        db_status = "error"
    
    return {
        "status": "healthy",
        "database": db_status,
        "embedding_model": "loaded",
        "gemini_model": "configured"
    }

@app.get("/")
async def root():
    """루트 경로 - API 상태"""
    return {
        "message": "🤖 RAG 질의응답 시스템 API가 실행 중입니다!",
        "endpoints": {
            "query": "/query - 질문 처리",
            "health": "/health - 서버 상태 확인",
            "models": "/models - 사용 가능한 모델 목록"
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=settings.host, port=settings.port)