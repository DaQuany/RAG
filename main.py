from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from pydantic_settings import BaseSettings
import os
from supabase import create_client, Client
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from typing import List, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor

# --- Configuration using Pydantic Settings ---
class Settings(BaseSettings):
    supabase_url: str
    supabase_key: str
    gemini_api_key: str
    host: str = "0.0.0.0"
    port: int = 8000

    class Config:
        env_file = ".env"

settings = Settings()

# --- Client Initialization ---
supabase: Client = create_client(settings.supabase_url, settings.supabase_key)

genai.configure(api_key=settings.gemini_api_key)

# Initialize Gemini model
try:
    gemini_model = genai.GenerativeModel('gemini-1.5-flash')
except Exception as e:
    print(f"gemini-1.5-flash not available, trying gemini-1.5-pro: {e}")
    try:
        gemini_model = genai.GenerativeModel('gemini-1.5-pro')
    except Exception as e:
        print(f"gemini-1.5-pro also not available, trying the default model: {e}")
        gemini_model = genai.GenerativeModel('gemini-pro')

# Load embedding model
print("Loading embedding model...")
embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
print("Embedding model loaded.")

app = FastAPI()

# --- Middleware ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class QueryInput(BaseModel):
    question: str
    top_k: Optional[int] = 3

class QueryResponse(BaseModel):
    answer: str
    relevant_documents: List[dict]

# --- Thread Pool for CPU-bound tasks ---
executor = ThreadPoolExecutor(max_workers=os.cpu_count())

# --- Helper Functions ---
def get_embedding(text: str) -> List[float]:
    """Converts text to a vector."""
    return embedding_model.encode(text).tolist()

async def check_available_models():
    """Checks the list of available Gemini models."""
    try:
        models = genai.list_models()
        available_models = []
        for model in models:
            if 'generateContent' in model.supported_generation_methods:
                available_models.append(model.name)
        return available_models
    except Exception as e:
        print(f"Failed to retrieve model list: {e}")
        return []

# --- API Endpoints ---
@app.on_event("startup")
async def startup_event():
    """Prints a guide message when the server starts."""
    print("=" * 60)
    print("ü§ñ RAG Question-Answering System has started!")
    print("=" * 60)
    
    # Check available models
    available_models = await check_available_models()
    if available_models:
        print("‚úÖ Available Gemini Models:")
        for model in available_models[:3]:  # Displaying only the first 3
            print(f"   - {model}")
    
    print("\nüìã Check Supabase Setup:")
    print("   - Please ensure the 'documents' table and 'vector' extension are enabled.")
    print("   - If necessary, run the following SQL:")
    print("""
-- 1. Enable pg_vector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- 2. Create documents table
CREATE TABLE IF NOT EXISTS documents (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    content TEXT NOT NULL,
    embedding VECTOR(768),
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- 3. Create function for vector search
CREATE OR REPLACE FUNCTION match_documents (
  query_embedding VECTOR(768),
  match_threshold FLOAT,
  match_count INT
)
RETURNS TABLE (
  id BIGINT,
  content TEXT,
  metadata JSONB,
  similarity FLOAT
)
LANGUAGE sql STABLE
AS $$
  SELECT
    d.id,
    d.content,
    d.metadata,
    1 - (d.embedding <=> query_embedding) AS similarity
  FROM documents d
  WHERE 1 - (d.embedding <=> query_embedding) > match_threshold
  ORDER BY similarity DESC
  LIMIT match_count;
$$;
    """)
    print("\nüåê Server Running:")
    print(f"   - Backend: http://localhost:{settings.port}")
    print("   - Frontend: Please open the index.html file in your browser")
    print("=" * 60)

@app.post("/query", response_model=QueryResponse)
async def query_documents(query: QueryInput):
    """Generate an answer to a question."""
    try:
        # 1. Convert the question to a vector
        current_loop = asyncio.get_running_loop()
        query_embedding = await current_loop.run_in_executor(executor, get_embedding, query.question)
        
        # 2. Search for relevant documents in Supabase
        try:
            response = supabase.rpc('match_documents', {
                'query_embedding': query_embedding,
                'match_threshold': 0.3,  # Lower threshold to return more results
                'match_count': query.top_k
            }).execute()
            
            top_docs = response.data if response.data else []
        except Exception as db_error:
            print(f"Database search error: {db_error}")
            # Handle database error with an empty document list
            top_docs = []
        
        # 3. If no documents are found, provide a general AI response
        if not top_docs:
            print("No relevant documents found. Providing a general AI response.")
            
            # AI response for a general question
            general_prompt = f"""You are a helpful AI assistant.
Please provide an accurate and useful answer to the user's question in English.

Question: {query.question}

Answer:"""
            
            try:
                gemini_response = gemini_model.generate_content(general_prompt)
                answer_text = gemini_response.text
            except Exception as gemini_error:
                print(f"Gemini API error: {gemini_error}")
                answer_text = "Sorry, there is a temporary issue with the AI service. Please try again later."
            
            return QueryResponse(
                answer=answer_text,
                relevant_documents=[]
            )
        
        # 4. If relevant documents are found, provide a context-based response
        context = "\n\n---\n\n".join([doc['content'] for doc in top_docs])
        
        # RAG-based prompt
        rag_prompt = f"""You are an AI assistant that answers questions based on the provided document content.

**Document Content:**
{context}

**Question:**
{query.question}

**Instructions:**
- You must answer based only on the 'Document Content' provided above.
- If the content is not in the document, reply with "The relevant content is not found in the document."
- Write the answer clearly and concisely in English.
- Include specific information if possible.

**Answer:**
"""
        
        try:
            gemini_response = gemini_model.generate_content(rag_prompt)
            answer_text = gemini_response.text
        except Exception as gemini_error:
            print(f"Gemini API error: {gemini_error}")
            answer_text = f"An error occurred while generating the AI response. Please check the reference document content.\n\nReference Documents:\n{context[:300]}..."
        
        return QueryResponse(
            answer=answer_text,
            relevant_documents=top_docs
        )
        
    except Exception as e:
        print(f"Error processing query: {e}")
        # Provide a default response on a general error
        return QueryResponse(
            answer="Sorry, there is a temporary issue with the service. Please try again later.",
            relevant_documents=[]
        )

@app.get("/models")
async def get_available_models():
    """Returns the list of available Gemini models."""
    try:
        available_models = await check_available_models()
        return {"available_models": available_models}
    except Exception as e:
        return {"error": f"Failed to retrieve model list: {str(e)}"}

@app.get("/health")
async def health_check():
    """Checks the server status."""
    try:
        # Test Supabase connection
        supabase.table("documents").select("count", count="exact").limit(1).execute()
        db_status = "healthy"
    except:
        db_status = "error"
    
    return {
        "status": "healthy",
        "database": db_status,
        "embedding_model": "loaded",
        "gemini_model": "configured"
    }

@app.get("/")
async def root():
    """Root path - API status"""
    return {
        "message": "ü§ñ RAG Question-Answering System API is running!",
        "endpoints": {
            "query": "/query - Process a question",
            "health": "/health - Check server status",
            "models": "/models - List available models"
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=settings.host, port=settings.port)